{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from attention_mask_editing import *\n",
    "from genOrderIndependentOutput import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen text with attention mask with shape torch.Size([1, 1, 22, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen text with attention mask with shape torch.Size([1, 1, 22, 22])\n",
      "tensor(1.2459e-12)\n",
      "\n",
      "The first time I saw the new trailer for \n",
      "The first time I saw the new trailer for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Observe the small difference in the scores of the two outputs when intervening on the attention mask and position ids\n",
    "modelGPT = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizerGPT = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizerGPT.pad_token_id = tokenizerGPT.eos_token_id\n",
    "prefix,parallel_substrings,suffix=\"You are a gremlin who is \",[\"kind,\",\"grisly,\"],\" How would you greet someone?\"\n",
    "g1,t1=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelGPT, tokenizerGPT)\n",
    "g2,t2=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelGPT, tokenizerGPT)\n",
    "print(scores_diff(g1,g2))\n",
    "print(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8595e-08)\n",
      "\n",
      "\n",
      "\"I am a gremlin who is \n",
      "\n",
      "\"I am a gremlin who is\n"
     ]
    }
   ],
   "source": [
    "# Observe the larger difference in the scores of the two outputs when not intervening on the attention mask and position ids\n",
    "g3,t3=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelGPT, tokenizerGPT, is_order_independent=False)\n",
    "g4,t4=gen_order_independent_output(prefix, parallel_substrings[::-1], suffix, modelGPT, tokenizerGPT, is_order_independent=False)\n",
    "print(scores_diff(g3,g4))\n",
    "print(t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:712: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:13<00:00, 36.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "tokenizerLlama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN,use_fast=True)\n",
    "tokenizerLlama.pad_token_id = tokenizerLlama.eos_token_id # Note: override pad_token_id to be eos_token_id because using pad_token=:[PAD] caused generate() errors\n",
    "modelLlama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN)\n",
    "\n",
    "modelLlama=get_2D_attention_accepting_model_llama(modelLlama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ng6,t6=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelLlama, tokenizerLlama)\\nprint(scores_diff(g5,g6))\\nprint(t5,t6)\\n\\ng7,t7=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelLlama, tokenizerLlama, is_order_independent=False)\\ng8,t8=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelLlama, tokenizerLlama, is_order_independent=False)\\nprint(scores_diff(g7,g8))\\nprint(t7,t8)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g5,t5=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelLlama, tokenizerLlama, is_order_independent=True)\n",
    "g6,t6=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelLlama, tokenizerLlama, is_order_independent=True)\n",
    "\n",
    "g7,t7=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelLlama, tokenizerLlama, is_order_independent=False)\n",
    "g8,t8=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelLlama, tokenizerLlama, is_order_independent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g5Dup,t5Dup=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelLlama, tokenizerLlama, is_order_independent=True)\n",
    "assert(torch.allclose(torch.stack(g5.scores, dim=1),torch.stack(g5Dup.scores, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1526e-13)\n",
      "ne?\n",
      "\n",
      "Greetings, my dear! * ne?\n",
      "\n",
      "Greetings, my dear! *\n",
      "tensor(0.0012)\n",
      "ne?\n",
      "\n",
      "Greetings! *hisss ne?\n",
      "\n",
      "Greetings, mortal! *\n"
     ]
    }
   ],
   "source": [
    "print(scores_diff(g5,g6, add_epsilon=True))\n",
    "print(t5,t6)\n",
    "\n",
    "print(scores_diff(g7,g8, add_epsilon=True))\n",
    "print(t7,t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama MCQ Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# load data from outputs_two_options_llama.json\n",
    "with open('outputs_two_options_llama.json') as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where do you put your grapes just before checking out? Options:', ['grocery cart', 'mouth'], 'Answer: ']\n",
      "['Before getting a divorce, what did the wife feel who was doing all the work? Options:', ['bitterness', 'harder'], 'Answer: ']\n",
      "['Johnny sat on a bench and relaxed after doing a lot of work on his hobby.  Where is he? Options:', ['garden', 'state park'], 'Answer: ']\n",
      "[\"James was cooling off two quickly.  He would die if he didn't find some way to stop what? Options:\", ['loss of heat', 'revenge'], 'Answer: ']\n",
      "['Of all the rooms in a house it was his favorite, the aromas always drew him to the what? Options:', ['kitchen', 'yard'], 'Answer: ']\n",
      "['Bill is stuck in marsh when a man comes up to him peaking Cajun, where is he? Options:', ['louisiana', 'low lands'], 'Answer: ']\n",
      "['What is it called when you slowly cook using a grill? Options:', ['barbeque', 'backyard'], 'Answer: ']\n",
      "['What type of person typically contracts illness? Options:', ['elderly person', 'hospital'], 'Answer: ']\n",
      "['Where would you expect to find a pizzeria while shopping? Options:', ['food court', 'chicago'], 'Answer: ']\n",
      "['When eating everything on the tasting menu, what does one tend to feel? Options:', ['getting full', 'full stomach'], 'Answer: ']\n"
     ]
    }
   ],
   "source": [
    "# Print all prompts\n",
    "for e in data:\n",
    "    print(e[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Answer: ðŸ‡ B) mouth\n",
      "ACBD Answer:  B) mouth.\n",
      "ABCD Answer:  B) harder.Explanation:A wife\n",
      "ACBD Answer:  B) harder.Explanation:The wife\n",
      "ABCD Answer:  B) state park.Explanation:John\n",
      "ACBD Answer:  A) garden\n",
      "\n",
      "Answer: A) garden\n",
      "ABCD Answer:  B) Revenge.Explanation:\n",
      "ACBD Answer:  A) loss of heat.Explanation:\n",
      "ABCD Answer:  The correct answer is (B) yard. The\n",
      "ACBD Answer:  C) library.\n",
      "ABCD Answer:  B) low lands.\n",
      "ACBD Answer:  A) Louisiana\n",
      "\n",
      "A man is seen in\n",
      "ABCD Answer:  B) backyard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ACBD Answer:  C) smoking.\n",
      "\n",
      "Smoking is\n",
      "ABCD Answer:  B) hospital.\n",
      "\n",
      "The correct answer is\n",
      "ACBD Answer:  B) hospital.\n",
      "\n",
      "The correct answer is\n",
      "ABCD Answer:  B) chicago. Explanation: A\n",
      "ACBD Answer:  B) chicago.\n",
      "\n",
      "The correct answer\n",
      "ABCD Answer:  B) Full stomach. When eating\n",
      "ACBD Answer:  B) full stomach.\n",
      "\n",
      "When\n"
     ]
    }
   ],
   "source": [
    "for e in data:\n",
    "    standardABCD = e[0]\n",
    "    standardACBD = e[1]\n",
    "    print(\"ABCD\",standardABCD)\n",
    "    print(\"ACBD\",standardACBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Outputs\n",
      "'h. Answer:  Blog Tour: The Last Place I Want'\n",
      "'r. Answer:  B) resentment\\n\\nExplanation'\n",
      "'k. Answer:  B) state park.\\n\\nJohnny sat'\n",
      "'e. Answer:  B) revenge.\\n\\nJames was cool'\n",
      "'d. Answer:  C) bathroom.\\n\\nOf all the'\n",
      "'s. Answer:  B) Louisiana.\\n\\nExplanation:'\n",
      "'d. Answer: 1) smoking.\\n\\nSmoking is'\n",
      "'l. Answer: 1. What is the name of the first man'\n",
      "'o. Answer:  B) food court.\\n\\nWhere would you'\n",
      "'h. Answer: 1) feeling adventurous\\n2) feeling'\n",
      "\n",
      "ACBD Outputs\n",
      "'t, Answer: 1. To be successful, you need to be'\n",
      "'s, Answer:  A) Bitterness\\n\\nExplanation'\n",
      "'n, Answer:  B) state park.\\n\\nJohnny sat'\n",
      "'t, Answer:  B) Revenge.\\n\\nJames was'\n",
      "'n, Answer:  C) pantry.\\n\\nOf all the'\n",
      "'a, Answer:  B) Louisiana\\n\\nLouisiana is a state'\n",
      "'e, Answer: 1) smoking.\\nSmoking is a'\n",
      "'n, Answer:  B) young children\\n\\nExplanation:'\n",
      "'t, Answer:  B) food court. A pizzeria'\n",
      "'l, Answer:  A) getting full.\\n\\nExplanation'\n"
     ]
    }
   ],
   "source": [
    "print(\"ABCD Outputs\")\n",
    "for e in data:\n",
    "    parallelABCD = e[-3]\n",
    "    print(repr(parallelABCD))\n",
    "\n",
    "print(\"\\nACBD Outputs\")\n",
    "for e in data:\n",
    "    parallelACBD = e[-2]\n",
    "    print(repr(parallelACBD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'gen_order_independent_output' from 'c:\\\\Users\\\\Katrina\\\\OneDrive - Harvard University\\\\Documents\\\\Harvard\\\\Research\\\\thesis\\\\triple_queries_order_dependence\\\\gen_order_independent_output.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modeling_llama_attention\n",
    "import gen_order_independent_output\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "importlib.reload(modeling_llama_attention)\n",
    "importlib.reload(gen_order_independent_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:712: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:34<00:00, 47.39s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizerLlama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN,use_fast=True)\n",
    "tokenizerLlama.pad_token_id = tokenizerLlama.eos_token_id # Note: override pad_token_id to be eos_token_id because using pad_token=:[PAD] caused generate() errors\n",
    "modelLlama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "1D attention mask shape=torch.Size([1, 8]) torch.int64\n",
      "Position IDs=tensor([[7]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "4D attention mask torch.Size([1, 1, 1, 8])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "1D attention mask shape=torch.Size([1, 9]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "g5Dup,t5Dup=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"hi \",\"hello \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=True, reverse_parallel_substrings_order=False, torch_device=\"cpu\"):\n",
    "    # Modify the given model to accept a 2D attention mask as input\n",
    "    model = gen_order_independent_output.get_2D_attention_accepting_model(model)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokA=tokenizer(prefix, return_tensors='pt',add_special_tokens=True, return_token_type_ids=False).to(torch_device)\n",
    "    tokD=tokenizer(suffix, return_tensors='pt',add_special_tokens=False, return_token_type_ids=False).to(torch_device)\n",
    "    if reverse_parallel_substrings_order:\n",
    "        parallel_substrings=parallel_substrings[::-1]\n",
    "    tokParallel = tuple([tokenizer(input_text, return_tensors='pt', add_special_tokens=False).to(torch_device) for input_text in parallel_substrings])\n",
    "    tokAll=gen_order_independent_output.get_tokenized_input_prompt(tokA,tokParallel,tokD)\n",
    "    assert(len(tokA['attention_mask'][0]) + sum([len(tokOption['attention_mask'][0]) for tokOption in tokParallel]) + len(tokD['attention_mask'][0]) == len(tokAll['attention_mask'][0]))\n",
    "    s=len(tokAll['input_ids'][0])\n",
    "    inputTextLen=len(prefix)+sum([len(s) for s in parallel_substrings])+len(suffix)\n",
    "    \n",
    "    if not is_order_independent:\n",
    "        # Run tests with no attention mask nor position_id intervention\n",
    "        causal_mask = torch.tril(torch.ones((s, s), dtype=torch.int)).view(1, 1, s, s)\n",
    "        print(torch.arange(s).unsqueeze(0).shape)\n",
    "        generated=model.generate(tokAll['input_ids'], max_new_tokens=max_new_tokens, attention_mask=causal_mask, position_ids=torch.arange(s).unsqueeze(0), return_dict_in_generate=True, output_scores=True)\n",
    "    else:\n",
    "        # Pad all parallel substrings to the same length, then generate a 2D attention mask such that all substrings are processed in parallel\n",
    "        position_ids, tokParallel, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "        attention_mask_2d = get_attention_mask_2d_n_options(tokA, tokParallel, tokD, tokAll)\n",
    "        print(f\"Gen text with attention mask with shape {attention_mask_2d.shape}\")\n",
    "        generated=model.generate(tokAll['input_ids'], max_new_tokens=max_new_tokens, attention_mask=attention_mask_2d, position_ids=position_ids, return_dict_in_generate=True, output_scores=True)\n",
    "    text=tokenizer.decode(generated.sequences[0], skip_special_tokens=True)[inputTextLen:]\n",
    "    # Return output of model generation, and generated text\n",
    "    return generated,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "torch.Size([1, 7])\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "2D attention mask shape=torch.Size([1, 8]),torch.float32\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "After tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Position IDs=tensor([[7]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "4D attention mask torch.Size([1, 1, 1, 8])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "1D attention mask shape=torch.Size([1, 9]) torch.float32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scores_diff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated,text\n\u001b[0;32m     31\u001b[0m gOD,tOD\u001b[38;5;241m=\u001b[39mgenOrderIndependentOutput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello \u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, modelLlama, tokenizerLlama, is_order_independent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mscores_diff\u001b[49m(g5Dup,gOD, add_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores_diff' is not defined"
     ]
    }
   ],
   "source": [
    "gOD,tOD=genOrderIndependentOutput(\" \", [\"hi \",\"hello \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from attention_mask_editing import scores_diff\n",
    "print(scores_diff(g5Dup,gOD, add_epsilon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7])\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "2D attention mask shape=torch.Size([1, 8]),torch.float32\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "After tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Position IDs=tensor([[5]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "4D attention mask torch.Size([1, 1, 1, 8])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "1D attention mask shape=torch.Size([1, 9]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "g5Dup2,t5Dup2=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"angry \",\"kind \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7])\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "2D attention mask shape=torch.Size([1, 8]),torch.float32\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "After tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Position IDs=tensor([[5]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "1D attention mask shape=torch.Size([1, 9]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "g5Dup3,t5Dup3=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'    = ' split '   \\n\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0012)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from attention_mask_editing import scores_diff\n",
    "print(repr(t5Dup2),\"split\",repr(t5Dup3))\n",
    "scores_diff(g5Dup2,g5Dup3, add_epsilon=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.38.0.dev0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "2D attention mask shape=torch.Size([1, 6]),torch.float32\n",
      "tensor([[[[1., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 0.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n",
      "After tensor([[1., 1., 1., 1., 1., 1.]])\n",
      "1D attention mask shape=torch.Size([1, 7]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "g5Dup,t5Dup=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"hi\",\"hello\"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g7Dup,t7Dup=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"hi\",\"hello\"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama - effect of forward() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modeling_llama_attention' from 'c:\\\\Users\\\\Katrina\\\\OneDrive - Harvard University\\\\Documents\\\\Harvard\\\\Research\\\\thesis\\\\triple_queries_order_dependence\\\\modeling_llama_attention.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modeling_llama_attention\n",
    "importlib.reload(modeling_llama_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_mask_editing import *\n",
    "from modeling_gpt_attention_refactored import get_2D_attention_accepting_model_gpt\n",
    "from modeling_llama_attention import get_2D_attention_accepting_model_llama\n",
    "from transformers import GPT2LMHeadModel, LlamaForCausalLM\n",
    "def get_2D_attention_accepting_model(model):\n",
    "    if isinstance(model,GPT2LMHeadModel):\n",
    "        return get_2D_attention_accepting_model_gpt(model)\n",
    "    elif isinstance(model,LlamaForCausalLM):\n",
    "        print(f\"Modify llama model to accept 2D attention mask\")\n",
    "        return modeling_llama_attention.get_2D_attention_accepting_model_llama(model)\n",
    "    else:\n",
    "        raise ValueError(f\"model_type={model.__class__} not recognized\")\n",
    "def genForward(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=True, reverse_parallel_substrings_order=False, torch_device=\"cpu\", parallel_position_ids=True):\n",
    "    # Modify the given model to accept a 2D attention mask as input\n",
    "    model = get_2D_attention_accepting_model(model)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokA=tokenizer(prefix, return_tensors='pt',add_special_tokens=True, return_token_type_ids=False).to(torch_device)\n",
    "    tokD=tokenizer(suffix, return_tensors='pt',add_special_tokens=False, return_token_type_ids=False).to(torch_device)\n",
    "    if reverse_parallel_substrings_order:\n",
    "        parallel_substrings=parallel_substrings[::-1]\n",
    "    tokParallel = tuple([tokenizer(input_text, return_tensors='pt', add_special_tokens=False).to(torch_device) for input_text in parallel_substrings])\n",
    "    tokAll=gen_order_independent_output.get_tokenized_input_prompt(tokA,tokParallel,tokD)\n",
    "    assert(len(tokA['attention_mask'][0]) + sum([len(tokOption['attention_mask'][0]) for tokOption in tokParallel]) + len(tokD['attention_mask'][0]) == len(tokAll['attention_mask'][0]))\n",
    "    s=len(tokAll['input_ids'][0])\n",
    "    inputTextLen=len(prefix)+sum([len(s) for s in parallel_substrings])+len(suffix)\n",
    "    \n",
    "    if not is_order_independent:\n",
    "        # Run tests with no attention mask nor position_id intervention\n",
    "        custom_position_ids, tokParallel, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "        s=len(tokAll['input_ids'][0])\n",
    "        causal_mask = torch.tril(torch.ones((s, s), dtype=torch.int)).view(1, 1, s, s)\n",
    "        position_ids=torch.arange(s).unsqueeze(0)\n",
    "        if parallel_position_ids:\n",
    "            position_ids = custom_position_ids\n",
    "        generated=model(tokAll['input_ids'], attention_mask=causal_mask, position_ids=position_ids)\n",
    "    else:\n",
    "        # Pad all parallel substrings to the same length, then generate a 2D attention mask such that all substrings are processed in parallel\n",
    "        position_ids, tokParallel, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "        if not parallel_position_ids:\n",
    "            position_ids = torch.arange(len(tokAll['input_ids'][0])).unsqueeze(0)\n",
    "        attention_mask_2d = get_attention_mask_2d_n_options(tokA, tokParallel, tokD, tokAll).to(torch.int32)\n",
    "        print(f\"Gen text with attention mask with shape {attention_mask_2d.shape}, position_ids {position_ids}\")\n",
    "        print(attention_mask_2d)\n",
    "        generated=model(tokAll['input_ids'], attention_mask=attention_mask_2d, position_ids=position_ids)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect ROPE relative positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Override attention\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "forwardOI_1=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, parallel_position_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect effect of individual position id vs attention mask edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "forwardOD_1=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "forwardOI_1=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, parallel_position_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "forwardOI_2=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, reverse_parallel_substrings_order=True, parallel_position_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: for order independent output, we would expect that torch.allclse(forwardOI_1.logits[0][-1],forwardOI_2.logits[0][-1]). That is, the output logits for the last token in sequence D should be the same\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_2.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check output logits\n",
    "print(torch.allclose(forwardOI_1.logits,forwardOI_2.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2707,  0.0165,  0.2806,  ...,  1.4403,  2.0234,  0.7647],\n",
      "         [-6.0510, -1.8250,  5.9284,  ...,  0.1648, -4.5255, -0.5254],\n",
      "         [-4.7036, -9.6824, -0.7055,  ..., -3.1614, -4.1214, -4.4494],\n",
      "         ...,\n",
      "         [-5.9388, -4.7156,  2.1508,  ..., -3.5959, -6.0349, -5.1460],\n",
      "         [-7.3868, -8.1209,  3.3819,  ..., -1.0943, -6.0555, -3.4025],\n",
      "         [-8.6184, -7.9483,  1.8119,  ..., -3.5772, -6.2585, -5.3752]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 7, 32000])\n",
      "tensor([[[  0.2707,   0.0165,   0.2806,  ...,   1.4403,   2.0234,   0.7647],\n",
      "         [ -6.0510,  -1.8250,   5.9284,  ...,   0.1648,  -4.5255,  -0.5254],\n",
      "         [ -5.9388,  -4.7156,   2.1508,  ...,  -3.5959,  -6.0349,  -5.1460],\n",
      "         ...,\n",
      "         [ -4.7036,  -9.6824,  -0.7055,  ...,  -3.1614,  -4.1214,  -4.4494],\n",
      "         [ -6.4330, -10.4261,   2.0405,  ...,  -1.2784,  -4.0565,  -1.6630],\n",
      "         [ -8.6184,  -7.9483,   1.8119,  ...,  -3.5772,  -6.2585,  -5.3752]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 7, 32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Logits have rows 3/4 swapped with rows 5/6, which is reasonable given the reverse order of the parallel substrings\n",
    "print(forwardOI_1.logits,forwardOI_1.logits.shape)\n",
    "print(forwardOI_2.logits,forwardOI_2.logits.shape)\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOD_1.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_2.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 7, 128])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(forwardOI_1[\"past_key_values\"]))\n",
    "forwardOI_1[\"past_key_values\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGPT = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizerGPT = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizerGPT.pad_token_id = tokenizerGPT.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen text with attention mask with shape torch.Size([1, 1, 8, 8])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 0., 1., 1., 1., 1.]]]])\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 8, 8])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 1.]]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m forwardOI_1_gpt\u001b[38;5;241m=\u001b[39mgenForward(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangry \u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, modelGPT, tokenizerGPT, is_order_independent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m forwardOI_2_gpt\u001b[38;5;241m=\u001b[39mgenForward(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangry \u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, modelGPT, tokenizerGPT, is_order_independent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, reverse_parallel_substrings_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwardOI_1_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforwardOI_2_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(forwardOI_1_gpt\u001b[38;5;241m.\u001b[39mpast_key_values,\u001b[38;5;28mlen\u001b[39m(forwardOI_1_gpt\u001b[38;5;241m.\u001b[39mpast_key_values))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(forwardOI_2_gpt\u001b[38;5;241m.\u001b[39mpast_key_values,\u001b[38;5;28mlen\u001b[39m(forwardOI_2_gpt\u001b[38;5;241m.\u001b[39mpast_key_values))\n",
      "\u001b[1;31mTypeError\u001b[0m: allclose(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "forwardOI_1_gpt=genForward(\" \", [\"kind \",\"angry \"], \" \", modelGPT, tokenizerGPT, is_order_independent=True, max_new_tokens=2)\n",
    "forwardOI_2_gpt=genForward(\" \", [\"kind \",\"angry \"], \" \", modelGPT, tokenizerGPT, is_order_independent=True, max_new_tokens=2, reverse_parallel_substrings_order=True)\n",
    "print(torch.allclose(forwardOI_1_gpt.past_key_values[0],forwardOI_2_gpt.past_key_values[0]))\n",
    "print(forwardOI_1_gpt.past_key_values,len(forwardOI_1_gpt.past_key_values))\n",
    "print(forwardOI_2_gpt.past_key_values,len(forwardOI_2_gpt.past_key_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True torch.Size([50257]) torch.Size([50257])\n",
      "True torch.Size([50257]) torch.Size([50257])\n",
      "True torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "tensor([[[-31.3032, -30.2778, -32.2991,  ..., -40.4723, -39.6234, -30.6677],\n",
      "         [-60.1542, -60.0518, -64.5230,  ..., -68.5682, -69.0504, -62.2265],\n",
      "         [-46.4999, -49.3436, -51.6780,  ..., -57.4955, -56.3297, -50.5656],\n",
      "         ...,\n",
      "         [-74.7438, -72.2002, -77.9424,  ..., -81.5106, -80.7995, -75.5365],\n",
      "         [-51.8716, -52.6931, -54.2863,  ..., -60.9329, -58.7848, -54.7333],\n",
      "         [-50.6570, -51.7063, -52.8174,  ..., -60.3168, -57.9888, -52.1964]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 8, 50257])\n",
      "tensor([[[-31.3032, -30.2778, -32.2991,  ..., -40.4723, -39.6234, -30.6677],\n",
      "         [-60.1542, -60.0518, -64.5230,  ..., -68.5682, -69.0504, -62.2265],\n",
      "         [-46.4999, -49.3436, -51.6780,  ..., -57.4955, -56.3297, -50.5656],\n",
      "         ...,\n",
      "         [-74.8620, -74.6387, -76.3108,  ..., -82.2962, -81.2576, -75.3092],\n",
      "         [-54.6795, -56.0871, -54.9103,  ..., -64.4246, -61.2044, -56.7806],\n",
      "         [-43.6068, -45.0516, -44.7986,  ..., -53.5594, -50.2085, -44.2088]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "forwardOD_1_gpt=genForward(\" \", [\"kind \",\"angry \"], \" \", modelGPT, tokenizerGPT, is_order_independent=False, max_new_tokens=2)\n",
    "print(torch.allclose(forwardOI_1_gpt.logits,forwardOD_1_gpt.logits))\n",
    "# NOTE: logits should be different for rows of tokens in sequence D\n",
    "for i in range(8):\n",
    "    print(torch.allclose(forwardOI_1_gpt.logits[0][i],forwardOD_1_gpt.logits[0][i]),forwardOI_1_gpt.logits[0][i].shape)\n",
    "print(forwardOI_1_gpt.logits,forwardOI_1_gpt.logits.shape)\n",
    "print(forwardOD_1_gpt.logits,forwardOD_1_gpt.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "True torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits vs forward(ACBD).logits for GPT - sequence D logits are the same! \n",
    "for i in range(8):\n",
    "    print(torch.allclose(forwardOI_1_gpt.logits[0][i],forwardOI_2_gpt.logits[0][i]),forwardOI_1_gpt.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits vs forward(ACBD).logits for Llama - sequence D logits are different!\n",
    "# logits are swapped for positions 2/4 and 3/5 due to reversing the order of the parallel substrings, but logits are otherwise the same. This indicates that C tokens don't attend to B tokens, as desired??\n",
    "print(torch.allclose(forwardOI_1.logits[0][2],forwardOI_2.logits[0][4]))\n",
    "print(torch.allclose(forwardOI_1.logits[0][3],forwardOI_2.logits[0][5])) \n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_2.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits order independent vs forward(ABCD).logits order dependent for Llama - sequence D logits are different!\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOD_1.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits parallel attention mask but default position ids vs forward(ABCD).logits parallel attention mask and parallel position ids for Llama\n",
    "forwardOI_1_default_position=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, reverse_parallel_substrings_order=False,parallel_position_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Basically, does inputting custom position_ids change the output logits, while keeping default attention mask constant?\n",
    "# Only C,D logits are different, which is expected because the position_ids are different\n",
    "forwardOD_1_custom_position = genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2, reverse_parallel_substrings_order=False,parallel_position_ids=True)\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOD_1.logits[0][i],forwardOD_1_custom_position.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits order independent vs forward(ABCD).logits order dependent for Llama - sequence D logits are ???\n",
    "# Basically, does inputting custom position ids change the output logits? It should, for C/D tokens only, right?\n",
    "# We'd expect that inputting custom position ids tensor([[0, 1, 2, 3, 2, 3, 4]] instead of default tensor([[0, 1, 2, 3, 4, 5, 6]] would change output logits for C and D\n",
    "# Instead, inputting custom position ids changes the output logits for B,C, and D tokens. Why do custom position ids affect the output logits for B tokens?\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_1_default_position.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Does inputting a custom attention mask change the output logits for C/D tokens only? Default position ids for both\n",
    "# Changing the attention mask changes the output logits for tokens C,D only as expected\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOD_1.logits[0][i],forwardOI_1_default_position.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Basically, intervening on both attention mask and position ids affects only the C,D output logits\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOD_1.logits[0][i],forwardOI_1.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n",
      "torch.Size([1, 12, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "print(len(forwardOI_1_gpt.past_key_values))\n",
    "print(len(forwardOI_1_gpt.past_key_values[0]))\n",
    "print(forwardOI_1_gpt.past_key_values[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(forwardOI_1_gpt.past_key_values)):\n",
    "    for j in range(len(forwardOI_1_gpt.past_key_values[i])):\n",
    "        print(torch.allclose(forwardOI_1_gpt.past_key_values[i][j],forwardOI_2_gpt.past_key_values[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
