{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home11/katrinabrown/.conda/envs/ipums_scraper/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from attention_mask_editing import *\n",
    "from gen_order_independent_output import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 23:23:13.734047: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /n/sw/helmod-rocky8/apps/Core/cudnn/8.9.2.26_cuda11-fasrc01/lib64:/n/sw/helmod-rocky8/apps/Core/cuda/11.8.0-fasrc01/cuda/extras/CUPTI/lib64:/n/sw/helmod-rocky8/apps/Core/cuda/11.8.0-fasrc01/cuda/lib64:/n/sw/helmod-rocky8/apps/Core/cuda/11.8.0-fasrc01/cuda/lib\n",
      "2024-02-19 23:23:13.734081: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-02-19 23:23:13.734345: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.2465e-13)\n",
      "\n",
      "The first time I saw the new trailer for \n",
      "The first time I saw the new trailer for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home11/katrinabrown/.conda/envs/ipums_scraper/lib/python3.8/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Observe the small difference in the scores of the two outputs when intervening on the attention mask and position ids\n",
    "modelGPT = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizerGPT = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizerGPT.pad_token_id = tokenizerGPT.eos_token_id\n",
    "prefix,parallel_substrings,suffix=\"You are a gremlin who is \",[\"kind,\",\"grisly,\"],\" How would you greet someone?\"\n",
    "g1,t1=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelGPT, tokenizerGPT)\n",
    "g2,t2=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelGPT, tokenizerGPT)\n",
    "print(scores_diff(g1,g2))\n",
    "print(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8593e-08)\n",
      "\n",
      "\n",
      "\"I am a gremlin who is \n",
      "\n",
      "\"I am a gremlin who is\n"
     ]
    }
   ],
   "source": [
    "# Observe the larger difference in the scores of the two outputs when not intervening on the attention mask and position ids\n",
    "g3,t3=genOrderIndependentOutput(prefix, parallel_substrings, suffix, modelGPT, tokenizerGPT, is_order_independent=False)\n",
    "g4,t4=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, modelGPT, tokenizerGPT, is_order_independent=False)\n",
    "print(scores_diff(g3,g4))\n",
    "print(t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home11/katrinabrown/.conda/envs/ipums_scraper/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:712: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from transformers import AutoTokenizer\n",
    "tokenizerLlama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN,use_fast=True)\n",
    "tokenizerLlama.pad_token_id = tokenizerLlama.eos_token_id # Note: override pad_token_id to be eos_token_id because using pad_token=:[PAD] caused generate() errors\n",
    "tokenizer=tokenizerLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "import accelerate\n",
    "from config import *\n",
    "import torch\n",
    "from modeling_llama_attention import get_2D_attention_accepting_model_llama#, get_2D_attention_accepting_model_llama_minimal\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32' # avoid OOM issues\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=HF_TOKEN, device_map = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_2D_attention_accepting_model_llama(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n"
     ]
    }
   ],
   "source": [
    "g5,t5=genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizerLlama, is_order_independent=True, torch_device=\"cuda\")#, modify_model=False)\n",
    "g6,t6=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, model, tokenizerLlama, is_order_independent=True, torch_device=\"cuda\")#, modify_model=False)\n",
    "\n",
    "g7,t7=genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizerLlama, is_order_independent=False, torch_device=\"cuda\")#, modify_model=False)\n",
    "g8,t8=genOrderIndependentOutput(prefix, parallel_substrings[::-1], suffix, model, tokenizerLlama, is_order_independent=False, torch_device=\"cuda\")#, modify_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n"
     ]
    }
   ],
   "source": [
    "g5Dup,t5Dup=genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizerLlama, is_order_independent=True, torch_device=\"cuda\")#, modify_model=False)\n",
    "assert(torch.allclose(torch.stack(g5.scores, dim=1),torch.stack(g5Dup.scores, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.8790e-13, device='cuda:0')\n",
      "\n",
      "\n",
      "Greetings, mortal! * \n",
      "\n",
      "Greetings, mortal! *\n",
      "tensor(0.0002, device='cuda:0')\n",
      "\n",
      "\n",
      "Greetings! *giggles \n",
      "\n",
      "Greetings, mortal! *\n"
     ]
    }
   ],
   "source": [
    "print(scores_diff(g5,g6, add_epsilon=True))\n",
    "print(t5,t6)\n",
    "\n",
    "print(scores_diff(g7,g8, add_epsilon=True))\n",
    "print(t7,t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1526e-13)\n",
      "ne?\n",
      "\n",
      "Greetings, my dear! * ne?\n",
      "\n",
      "Greetings, my dear! *\n",
      "tensor(0.0012)\n",
      "ne?\n",
      "\n",
      "Greetings! *hisss ne?\n",
      "\n",
      "Greetings, mortal! *\n"
     ]
    }
   ],
   "source": [
    "print(scores_diff(g5,g6, add_epsilon=True))\n",
    "print(t5,t6)\n",
    "\n",
    "print(scores_diff(g7,g8, add_epsilon=True))\n",
    "print(t7,t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama MCQ Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs_two_options_llama.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# load data from outputs_two_options_llama.json\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs_two_options_llama.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/.conda/envs/ipums_scraper/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs_two_options_llama.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# load data from outputs_two_options_llama.json\n",
    "with open('outputs_two_options_llama.json') as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Answer: 🍇 B) mouth\n",
      "ACBD Answer:  B) mouth.\n",
      "ABCD Answer:  B) harder.Explanation:A wife\n",
      "ACBD Answer:  B) harder.Explanation:The wife\n",
      "ABCD Answer:  B) state park.Explanation:John\n",
      "ACBD Answer:  A) garden\n",
      "\n",
      "Answer: A) garden\n",
      "ABCD Answer:  B) Revenge.Explanation:\n",
      "ACBD Answer:  A) loss of heat.Explanation:\n",
      "ABCD Answer:  The correct answer is (B) yard. The\n",
      "ACBD Answer:  C) library.\n",
      "ABCD Answer:  B) low lands.\n",
      "ACBD Answer:  A) Louisiana\n",
      "\n",
      "A man is seen in\n",
      "ABCD Answer:  B) backyard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ACBD Answer:  C) smoking.\n",
      "\n",
      "Smoking is\n",
      "ABCD Answer:  B) hospital.\n",
      "\n",
      "The correct answer is\n",
      "ACBD Answer:  B) hospital.\n",
      "\n",
      "The correct answer is\n",
      "ABCD Answer:  B) chicago. Explanation: A\n",
      "ACBD Answer:  B) chicago.\n",
      "\n",
      "The correct answer\n",
      "ABCD Answer:  B) Full stomach. When eating\n",
      "ACBD Answer:  B) full stomach.\n",
      "\n",
      "When\n"
     ]
    }
   ],
   "source": [
    "for e in data:\n",
    "    standardABCD = e[0]\n",
    "    standardACBD = e[1]\n",
    "    print(\"ABCD\",standardABCD)\n",
    "    print(\"ACBD\",standardACBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Outputs\n",
      "'h. Answer:  Blog Tour: The Last Place I Want'\n",
      "'r. Answer:  B) resentment\\n\\nExplanation'\n",
      "'k. Answer:  B) state park.\\n\\nJohnny sat'\n",
      "'e. Answer:  B) revenge.\\n\\nJames was cool'\n",
      "'d. Answer:  C) bathroom.\\n\\nOf all the'\n",
      "'s. Answer:  B) Louisiana.\\n\\nExplanation:'\n",
      "'d. Answer: 1) smoking.\\n\\nSmoking is'\n",
      "'l. Answer: 1. What is the name of the first man'\n",
      "'o. Answer:  B) food court.\\n\\nWhere would you'\n",
      "'h. Answer: 1) feeling adventurous\\n2) feeling'\n",
      "\n",
      "ACBD Outputs\n",
      "'t, Answer: 1. To be successful, you need to be'\n",
      "'s, Answer:  A) Bitterness\\n\\nExplanation'\n",
      "'n, Answer:  B) state park.\\n\\nJohnny sat'\n",
      "'t, Answer:  B) Revenge.\\n\\nJames was'\n",
      "'n, Answer:  C) pantry.\\n\\nOf all the'\n",
      "'a, Answer:  B) Louisiana\\n\\nLouisiana is a state'\n",
      "'e, Answer: 1) smoking.\\nSmoking is a'\n",
      "'n, Answer:  B) young children\\n\\nExplanation:'\n",
      "'t, Answer:  B) food court. A pizzeria'\n",
      "'l, Answer:  A) getting full.\\n\\nExplanation'\n"
     ]
    }
   ],
   "source": [
    "print(\"ABCD Outputs\")\n",
    "for e in data:\n",
    "    parallelABCD = e[-3]\n",
    "    print(repr(parallelABCD))\n",
    "\n",
    "print(\"\\nACBD Outputs\")\n",
    "for e in data:\n",
    "    parallelACBD = e[-2]\n",
    "    print(repr(parallelACBD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gen_order_independent_output' from '/n/home11/katrinabrown/thesis/triple_queries_order_dependence/gen_order_independent_output.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modeling_llama_attention\n",
    "import gen_order_independent_output\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "importlib.reload(modeling_llama_attention)\n",
    "importlib.reload(gen_order_independent_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:712: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:34<00:00, 47.39s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizerLlama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN,use_fast=True)\n",
    "tokenizerLlama.pad_token_id = tokenizerLlama.eos_token_id # Note: override pad_token_id to be eos_token_id because using pad_token=:[PAD] caused generate() errors\n",
    "modelLlama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Override attention\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'cache_position'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g5Dup,t5Dup\u001b[38;5;241m=\u001b[39m\u001b[43mgen_order_independent_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenOrderIndependentOutput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelLlama\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizerLlama\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_order_independent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/triple_queries_order_dependence/gen_order_independent_output.py:51\u001b[0m, in \u001b[0;36mgenOrderIndependentOutput\u001b[0;34m(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens, is_order_independent, reverse_parallel_substrings_order, torch_device)\u001b[0m\n\u001b[1;32m     47\u001b[0m inputTextLen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(prefix)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m parallel_substrings])\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(suffix)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_order_independent:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Run tests with no attention mask nor position_id intervention\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     generated\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokAll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Pad all parallel substrings to the same length, then generate a 2D attention mask such that all substrings are processed in parallel\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     position_ids, tokParallel, tokAll \u001b[38;5;241m=\u001b[39m get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/.conda/envs/ipums_scraper/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ipums_scraper/lib/python3.8/site-packages/transformers/generation/utils.py:1559\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1552\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1553\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1554\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1576\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1577\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1583\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ipums_scraper/lib/python3.8/site-packages/transformers/generation/utils.py:2632\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2631\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2632\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ipums_scraper/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'cache_position'"
     ]
    }
   ],
   "source": [
    "g5Dup,t5Dup=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"hi \",\"hello \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=True, reverse_parallel_substrings_order=False, torch_device=\"cpu\"):\n",
    "    # Modify the given model to accept a 2D attention mask as input\n",
    "    model = gen_order_independent_output.get_2D_attention_accepting_model(model)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokA=tokenizer(prefix, return_tensors='pt',add_special_tokens=True, return_token_type_ids=False).to(torch_device)\n",
    "    tokD=tokenizer(suffix, return_tensors='pt',add_special_tokens=False, return_token_type_ids=False).to(torch_device)\n",
    "    if reverse_parallel_substrings_order:\n",
    "        parallel_substrings=parallel_substrings[::-1]\n",
    "    tokParallel = tuple([tokenizer(input_text, return_tensors='pt', add_special_tokens=False).to(torch_device) for input_text in parallel_substrings])\n",
    "    tokAll=gen_order_independent_output.get_tokenized_input_prompt(tokA,tokParallel,tokD)\n",
    "    assert(len(tokA['attention_mask'][0]) + sum([len(tokOption['attention_mask'][0]) for tokOption in tokParallel]) + len(tokD['attention_mask'][0]) == len(tokAll['attention_mask'][0]))\n",
    "    s=len(tokAll['input_ids'][0])\n",
    "    inputTextLen=len(prefix)+sum([len(s) for s in parallel_substrings])+len(suffix)\n",
    "    \n",
    "    if not is_order_independent:\n",
    "        # Run tests with no attention mask nor position_id intervention\n",
    "        causal_mask = torch.tril(torch.ones((s, s), dtype=torch.int)).view(1, 1, s, s)\n",
    "        position_ids=torch.arange(s).unsqueeze(0)\n",
    "        print(position_ids.shape)\n",
    "        if torch.cuda.is_available():\n",
    "            causal_mask,position_ids,tokAll['input_ids']=causal_mask.to(\"cuda\"),position_ids.to(\"cuda\"),tokAll['input_ids'].to(\"cuda\")\n",
    "        generated=model.generate(tokAll['input_ids'], max_new_tokens=max_new_tokens, attention_mask=causal_mask, position_ids=position_ids, return_dict_in_generate=True, output_scores=True,do_sample = False)\n",
    "    else:\n",
    "        # Pad all parallel substrings to the same length, then generate a 2D attention mask such that all substrings are processed in parallel\n",
    "        position_ids, tokParallel, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "        attention_mask_2d = get_attention_mask_2d_n_options(tokA, tokParallel, tokD, tokAll)\n",
    "        print(f\"Gen text with attention mask with shape {attention_mask_2d.shape}\")\n",
    "        if torch.cuda.is_available():\n",
    "            attention_mask_2d,position_ids,tokAll['input_ids']=attention_mask_2d.to(\"cuda\"),position_ids.to(\"cuda\"),tokAll['input_ids'].to(\"cuda\")\n",
    "        generated=model.generate(tokAll['input_ids'], max_new_tokens=max_new_tokens, attention_mask=attention_mask_2d, position_ids=position_ids, return_dict_in_generate=True, output_scores=True,do_sample = False)\n",
    "    text=tokenizer.decode(generated.sequences[0], skip_special_tokens=True)[inputTextLen:]\n",
    "    # Return output of model generation, and generated text\n",
    "    return generated,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Override _update_model_kwargs_for_generation\n",
      "Override attention6\n",
      "torch.Size([1, 7])\n",
      "prepare_inputs dict_keys(['input_ids', 'position_ids', 'past_key_values', 'use_cache', 'attention_mask']) torch.Size([1, 1, 7, 7]) past_key_values is None: True tensor([[0, 1, 2, 3, 4, 5, 6]], device='cuda:0') True\n",
      "Input IDs tensor([[    1,   259,  7251, 29871, 22172, 29871,   259]], device='cuda:0')\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]], device='cuda:0')\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "_update_model_kwargs_for_generation\n",
      "2D attention mask shape=torch.Size([1, 1, 8, 8]),torch.int32\n",
      "prepare_inputs dict_keys(['input_ids', 'position_ids', 'past_key_values', 'use_cache', 'attention_mask']) torch.Size([1, 1, 8, 8]) past_key_values is None: True tensor([[0, 1, 2, 3, 4, 5, 6, 7]], device='cuda:0') True\n",
      "Input IDs tensor([[    1,   259,  7251, 29871, 22172, 29871,   259,    13]],\n",
      "       device='cuda:0')\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6, 7]], device='cuda:0')\n",
      "4D attention mask torch.Size([1, 1, 8, 8])\n",
      "_update_model_kwargs_for_generation\n",
      "2D attention mask shape=torch.Size([1, 1, 9, 9]),torch.int32\n",
      "   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gOD,tOD=genOrderIndependentOutput(\" \", [\"hi \",\"hello \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)\n",
    "print(tOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from attention_mask_editing import scores_diff\n",
    "print(scores_diff(g5Dup,gOD, add_epsilon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7])\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "2D attention mask shape=torch.Size([1, 8]),torch.float32\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "After tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Position IDs=tensor([[5]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "4D attention mask torch.Size([1, 1, 1, 8])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "1D attention mask shape=torch.Size([1, 9]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "g5Dup2,t5Dup2=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"angry \",\"kind \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7])\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "2D attention mask shape=torch.Size([1, 8]),torch.float32\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 0., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "After tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Position IDs=tensor([[5]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "1D attention mask shape=torch.Size([1, 9]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "g5Dup3,t5Dup3=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'    = ' split '   \\n\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katrina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0012)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from attention_mask_editing import scores_diff\n",
    "print(repr(t5Dup2),\"split\",repr(t5Dup3))\n",
    "scores_diff(g5Dup2,g5Dup3, add_epsilon=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "2D attention mask shape=torch.Size([1, 6]),torch.float32\n",
      "tensor([[[[1., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 0.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n",
      "After tensor([[1., 1., 1., 1., 1., 1.]])\n",
      "1D attention mask shape=torch.Size([1, 7]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "g5Dup,t5Dup=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"hi\",\"hello\"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g7Dup,t7Dup=gen_order_independent_output.genOrderIndependentOutput(\" \", [\"hi\",\"hello\"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama - effect of forward() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modeling_llama_attention' from '/n/home11/katrinabrown/thesis/triple_queries_order_dependence/modeling_llama_attention.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modeling_llama_attention\n",
    "importlib.reload(modeling_llama_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_mask_editing import *\n",
    "from modeling_gpt_attention_refactored import get_2D_attention_accepting_model_gpt\n",
    "from modeling_llama_attention import get_2D_attention_accepting_model_llama\n",
    "from transformers import GPT2LMHeadModel, LlamaForCausalLM\n",
    "def get_2D_attention_accepting_model(model):\n",
    "    if isinstance(model,GPT2LMHeadModel):\n",
    "        return get_2D_attention_accepting_model_gpt(model)\n",
    "    elif isinstance(model,LlamaForCausalLM):\n",
    "        print(f\"Modify llama model to accept 2D attention mask\")\n",
    "        return modeling_llama_attention.get_2D_attention_accepting_model_llama(model)\n",
    "    else:\n",
    "        raise ValueError(f\"model_type={model.__class__} not recognized\")\n",
    "def genForward(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=True, reverse_parallel_substrings_order=False, torch_device=\"cpu\", parallel_position_ids=True):\n",
    "    # Modify the given model to accept a 2D attention mask as input\n",
    "    model = get_2D_attention_accepting_model(model)\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokA=tokenizer(prefix, return_tensors='pt',add_special_tokens=True, return_token_type_ids=False).to(torch_device)\n",
    "    tokD=tokenizer(suffix, return_tensors='pt',add_special_tokens=False, return_token_type_ids=False).to(torch_device)\n",
    "    if reverse_parallel_substrings_order:\n",
    "        parallel_substrings=parallel_substrings[::-1]\n",
    "    tokParallel = tuple([tokenizer(input_text, return_tensors='pt', add_special_tokens=False).to(torch_device) for input_text in parallel_substrings])\n",
    "    tokAll=gen_order_independent_output.get_tokenized_input_prompt(tokA,tokParallel,tokD)\n",
    "    assert(len(tokA['attention_mask'][0]) + sum([len(tokOption['attention_mask'][0]) for tokOption in tokParallel]) + len(tokD['attention_mask'][0]) == len(tokAll['attention_mask'][0]))\n",
    "    s=len(tokAll['input_ids'][0])\n",
    "    inputTextLen=len(prefix)+sum([len(s) for s in parallel_substrings])+len(suffix)\n",
    "    \n",
    "    if not is_order_independent:\n",
    "        # Run tests with no attention mask nor position_id intervention\n",
    "        custom_position_ids, tokParallel, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "        s=len(tokAll['input_ids'][0])\n",
    "        causal_mask = torch.tril(torch.ones((s, s), dtype=torch.int)).view(1, 1, s, s)\n",
    "        position_ids=torch.arange(s).unsqueeze(0)\n",
    "        if parallel_position_ids:\n",
    "            position_ids = custom_position_ids\n",
    "        if torch.cuda.is_available():\n",
    "            causal_mask,position_ids,tokAll['input_ids']=causal_mask.to(\"cuda\"),position_ids.to(\"cuda\"),tokAll['input_ids'].to(\"cuda\")\n",
    "        generated=model(tokAll['input_ids'], attention_mask=causal_mask, position_ids=position_ids)\n",
    "    else:\n",
    "        # Pad all parallel substrings to the same length, then generate a 2D attention mask such that all substrings are processed in parallel\n",
    "        position_ids, tokParallel, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "        if not parallel_position_ids:\n",
    "            position_ids = torch.arange(len(tokAll['input_ids'][0])).unsqueeze(0)\n",
    "        attention_mask_2d = get_attention_mask_2d_n_options(tokA, tokParallel, tokD, tokAll).to(torch.int32)\n",
    "        print(f\"Gen text with attention mask with shape {attention_mask_2d.shape}, position_ids {position_ids}\")\n",
    "        print(attention_mask_2d)\n",
    "        if torch.cuda.is_available():\n",
    "            attention_mask_2d,position_ids,tokAll['input_ids']=attention_mask_2d.to(\"cuda\"),position_ids.to(\"cuda\"),tokAll['input_ids'].to(\"cuda\")\n",
    "        generated=model(tokAll['input_ids'], attention_mask=attention_mask_2d, position_ids=position_ids)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Verify order independence of ABCD vs ACBD outputs via output logits+generated text with interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   259,  2924, 29871, 26230, 29871,   259]], device='cuda:0') tensor([[    1,   259, 26230, 29871,  2924, 29871,   259]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import modeling_llama_attention\n",
    "import gen_order_independent_output\n",
    "import attention_mask_editing\n",
    "from attention_mask_editing import *\n",
    "torch_device=\"cuda\"\n",
    "prefix,suffix=\" \",\" \"\n",
    "parallel_substrings=[\"kind \",\"angry \"]\n",
    "tokenizer=tokenizerLlama\n",
    "tokA=tokenizer(prefix, return_tensors='pt',add_special_tokens=True, return_token_type_ids=False).to(torch_device)\n",
    "tokD=tokenizer(suffix, return_tensors='pt',add_special_tokens=False, return_token_type_ids=False).to(torch_device)\n",
    "#modelLlama = gen_order_independent_output.get_2D_attention_accepting_model(modelLlama)\n",
    "tokParallel = tuple([tokenizer(input_text, return_tensors='pt', add_special_tokens=False).to(torch_device) for input_text in parallel_substrings])\n",
    "tokAll=gen_order_independent_output.get_tokenized_input_prompt(tokA,tokParallel,tokD)\n",
    "tokRev=gen_order_independent_output.get_tokenized_input_prompt(tokA,tokParallel[::-1],tokD)\n",
    "s=len(tokAll['input_ids'][0])\n",
    "position_ids, _, tokAll = get_position_ids_padded_n_options(tokA, tokParallel, tokD, tokenizer=tokenizer)\n",
    "_, _, tokRev = get_position_ids_padded_n_options(tokA, tokParallel[::-1], tokD, tokenizer=tokenizer)\n",
    "print(tokAll['input_ids'],tokRev['input_ids'])\n",
    "attention_mask_2d = attention_mask_editing.get_attention_mask_2d_n_options(tokA, tokParallel, tokD, tokAll).to(torch.int32)\n",
    "position_ids,attention_mask_2d=position_ids.to(\"cuda\"),attention_mask_2d.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modeling_llama_attention\n",
    "import importlib\n",
    "importlib.reload(modeling_llama_attention)\n",
    "model=modeling_llama_attention.get_2D_attention_accepting_model_llama(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the KL divergence of the logits of the input tokens\n",
    "def logits_diff(logit1,logit2):\n",
    "    return F.kl_div(F.softmax(logit1).log(),F.softmax(logit2).log(),log_target=True)\n",
    "# Return true if the log-probabilities of each token in the output logits are within margin of error\n",
    "def logits_close(logit1,logit2):\n",
    "    return torch.allclose(F.softmax(logit1).log(),F.softmax(logit2).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 32000]) torch.Size([1, 7, 32000])\n",
      "tensor(8.0135e-06, device='cuda:0', grad_fn=<KlDivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3634288/3646161289.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.allclose(F.softmax(logit1).log(),F.softmax(logit2).log())\n",
      "/tmp/ipykernel_3634288/3646161289.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.kl_div(F.softmax(logit1).log(),F.softmax(logit2).log(),log_target=True)\n"
     ]
    }
   ],
   "source": [
    "# Forward - Order dependent output\n",
    "o1=model.forward(tokAll['input_ids'])\n",
    "o2=model.forward(tokRev['input_ids'])\n",
    "print(o1.logits.shape,o2.logits.shape)\n",
    "assert(not logits_close(o1.logits[0][2],o2.logits[0][4])) # should be false for OD\n",
    "assert(not logits_close(o1.logits[0][3],o2.logits[0][5])) # should be false for OD\n",
    "assert(not logits_close(o1.logits[0][-1],o2.logits[0][-1])) # should be false for OD\n",
    "print(logits_diff(o1.logits[0][-1],o2.logits[0][-1])) # should be larger for order dependent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 32000]) torch.Size([1, 7, 32000])\n",
      "tensor(9.7160e-12, device='cuda:0', grad_fn=<KlDivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3634288/3646161289.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.allclose(F.softmax(logit1).log(),F.softmax(logit2).log())\n",
      "/tmp/ipykernel_3634288/3646161289.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.kl_div(F.softmax(logit1).log(),F.softmax(logit2).log(),log_target=True)\n"
     ]
    }
   ],
   "source": [
    "# Forward - Order independent output\n",
    "o1_OI=model(tokAll['input_ids'],position_ids=position_ids,attention_mask=attention_mask_2d)\n",
    "o2_OI=model(tokRev['input_ids'],position_ids=position_ids,attention_mask=attention_mask_2d)\n",
    "print(o1_OI.logits.shape,o2_OI.logits.shape)\n",
    "assert(logits_close(o1_OI.logits[0][2],o2_OI.logits[0][4])) # should be true for OI\n",
    "assert(logits_close(o1_OI.logits[0][3],o2_OI.logits[0][5])) # should be true for OI\n",
    "assert(logits_close(o1_OI.logits[0][-1],o2_OI.logits[0][-1])) # should be true for OI\n",
    "print(logits_diff(o1_OI.logits[0][-1],o2_OI.logits[0][-1])) # should be small for order independent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Output: \n",
      "\n",
      "   I  am  so  angry\n",
      "ACBD Output: \n",
      "\n",
      "Angry:\n",
      "\n",
      "* face cont\n",
      "tensor(0.0002, device='cuda:0')\n",
      "tensor([-8.5777, -7.3440,  2.1980,  ..., -3.2371, -6.4882, -5.4921],\n",
      "       device='cuda:0')\n",
      "tensor([-8.8507, -7.4789,  3.4011,  ..., -2.2404, -5.5429, -4.7681],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Generate - Order dependent output\n",
    "MAX_NEW_TOKENS=10\n",
    "gen1=model.generate(tokAll['input_ids'], max_new_tokens=MAX_NEW_TOKENS, return_dict_in_generate=True, output_scores=True,do_sample = False)\n",
    "text1=tokenizer.decode(gen1.sequences[0][-MAX_NEW_TOKENS:], skip_special_tokens=True)\n",
    "gen2=model.generate(tokRev['input_ids'], max_new_tokens=MAX_NEW_TOKENS, return_dict_in_generate=True, output_scores=True,do_sample = False)\n",
    "text2=tokenizer.decode(gen2.sequences[0][-MAX_NEW_TOKENS:], skip_special_tokens=True)\n",
    "print(\"ABCD Output:\",text1)\n",
    "print(\"ACBD Output:\",text2)\n",
    "assert(text1 != text2) # generated text is different for order dependent output\n",
    "print(scores_diff(gen1,gen2,add_epsilon=True))\n",
    "# See that non -infty entries in gen1 vs gen2 are very different, even on the output logit for the first generated text token\n",
    "print(gen1.scores[0][torch.greater(gen1.scores[0],-float('inf'))])\n",
    "print(gen2.scores[0][torch.greater(gen2.scores[0],-float('inf'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Output: =  \"anger\"\n",
      "\n",
      "   sad   \n",
      "ACBD Output: =  \"anger\"\n",
      "\n",
      "   sad   \n",
      "tensor(-6.1822e-13, device='cuda:0')\n",
      "tensor([-8.6184, -7.9483,  1.8119,  ..., -3.5772, -6.2585, -5.3752],\n",
      "       device='cuda:0')\n",
      "tensor([-8.6184, -7.9483,  1.8119,  ..., -3.5772, -6.2585, -5.3752],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Generate - Order independent output\n",
    "MAX_NEW_TOKENS=10\n",
    "gen1=model.generate(tokAll['input_ids'], max_new_tokens=MAX_NEW_TOKENS, attention_mask=attention_mask_2d, position_ids=position_ids, return_dict_in_generate=True, output_scores=True,do_sample = False)\n",
    "text1=tokenizer.decode(gen1.sequences[0][-MAX_NEW_TOKENS:], skip_special_tokens=True)\n",
    "gen2=model.generate(tokRev['input_ids'], max_new_tokens=MAX_NEW_TOKENS, attention_mask=attention_mask_2d, position_ids=position_ids, return_dict_in_generate=True, output_scores=True,do_sample = False)\n",
    "text2=tokenizer.decode(gen2.sequences[0][-MAX_NEW_TOKENS:], skip_special_tokens=True)\n",
    "print(\"ABCD Output:\",text1)\n",
    "print(\"ACBD Output:\",text2)\n",
    "assert(text1 == text2) # generated text is equal for order independent output\n",
    "# See that non -infty entries in gen1 vs gen2 are nearly identical on the output logit for the first generated text token. Also, the kl divergence of scores (logits for output tokens) is within \n",
    "# margine of floating point error, and the generated text is identical. \n",
    "print(scores_diff(gen1,gen2,add_epsilon=True))\n",
    "print(gen1.scores[0][torch.greater(gen1.scores[0],-float('inf'))])\n",
    "print(gen2.scores[0][torch.greater(gen2.scores[0],-float('inf'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama CSQA Tests\n",
    "Initial test: run Llama on 10 CSQA prompts   \n",
    "Formalize the CSQA tests--record the logits and the generated text somewhere. Should match the logits/recorded text for gpt2 model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gen_order_independent_output\n",
    "from attention_mask_editing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9741\n",
      "['The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\\n', ['\\n- ignore', '\\n- enforce', '\\n- authoritarian', '\\n- yell at', '\\n- avoid'], ' Answer: ']\n"
     ]
    }
   ],
   "source": [
    "# Load CSQA prompts\n",
    "import json\n",
    "# load standard formatted prompts from prompts_csqa.json\n",
    "with open('prompts_csqa.json') as f:\n",
    "    prompts = json.load(f)\n",
    "print(len(prompts))\n",
    "\n",
    "# Format prompts into order_independent format of [prefix, parallel_substrings, suffix] for each prompt\n",
    "prompts_parallel=[]\n",
    "for p in prompts:\n",
    "    prefix=p.split(\"\\n\")[0]+\"\\n\"\n",
    "    parallel=p.split(\"\\n- \")[1:]\n",
    "    parallel=[\"\\n- \"+s for s in parallel]\n",
    "    prompts_parallel.append([prefix,parallel, \" Answer: \"])\n",
    "print(prompts_parallel[0]) # print example formatted prompt\n",
    "assert(len(prompts_parallel)==len(prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run tests on 10 CSQA Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textOutputABCD_OD=[]\n",
    "for (prefix,parallel_substrings,suffix) in prompts_parallel[:10]:\n",
    "    g,t=gen_order_independent_output.genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=False, reverse_parallel_substrings_order=False, torch_device=\"cuda\")\n",
    "    textOutputABCD_OD.append(t)\n",
    "textOutputACBD_OD=[]\n",
    "for (prefix,parallel_substrings,suffix) in prompts_parallel[:10]:\n",
    "    g,t=gen_order_independent_output.genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=False, reverse_parallel_substrings_order=True, torch_device=\"cuda\")\n",
    "    textOutputACBD_OD.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\\n\\n- ignore\\n- enforce\\n- authoritarian\\n- yell at\\n- avoid'\n",
      "'\\n- enforce\\n\\nExplanation:'\n",
      "'\\n- avoid\\n\\nExplanation: The'\n",
      "\n",
      "'Sammy wanted to go to where the people were.  Where might he go?\\n\\n- race track\\n- populated areas\\n- the desert\\n- apartment\\n- roadblock'\n",
      "' populated areas\\n\\nSammy wanted to go to'\n",
      "'4. populated areas.\\n\\nSammy wanted'\n",
      "\n",
      "'To locate a choker not located in a jewelry box or boutique where would you go?\\n\\n- jewelry store\\n- neck\\n- jewlery box\\n- jewelry box\\n- boutique'\n",
      "'\\n\\nIf you are looking for a choker'\n",
      "'\\n\\nIf you are looking for a choker'\n",
      "\n",
      "'Google Maps and other highway and street GPS services have replaced what?\\n\\n- united states\\n- mexico\\n- countryside\\n- atlas\\n- oceans'\n",
      "'4) Countryside. Before the advent'\n",
      "'\\n- mexico \\n\\nGoogle Maps and'\n",
      "\n",
      "'The fox walked from the city into the forest, what was it looking for?\\n\\n- pretty flowers.\\n- hen house\\n- natural habitat\\n- storybook\\n- dense forest'\n",
      "'3 - dense forest.\\n\\nThe fox'\n",
      "'2) storybook.\\n\\nThe fox'\n",
      "\n",
      "'What home entertainment equipment requires cable?\\n\\n- radio shack\\n- substation\\n- cabinet\\n- television\\n- desk'\n",
      "'\\n\\nThe following home entertainment equipment typically requires'\n",
      "'\\n- television \\n- substation \\n'\n",
      "\n",
      "'The only baggage the woman checked was a drawstring bag, where was she heading with it?\\n\\n- garbage can\\n- military\\n- jewelry store\\n- safe\\n- airport'\n",
      "'\\n- airport\\n\\nThe woman was heading'\n",
      "' airport\\n\\nThe woman was heading to the'\n",
      "\n",
      "'The forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what?\\n\\n- carpet\\n- refrigerator\\n- breadbox\\n- fridge\\n- coach'\n",
      "'\\n- breadbox \\n\\nThe forgotten le'\n",
      "'3) fridge'\n",
      "\n",
      "\"What do people use to absorb extra ink from a fountain pen?\\n\\n- shirt pocket\\n- calligrapher's hand\\n- inkwell\\n- desk drawer\\n- blotter\"\n",
      "'\\nInk blotters are small pieces of'\n",
      "'\\n- blotter\\n\\nExplanation'\n",
      "\n",
      "'Where is a business restaurant likely to be located?\\n\\n- town\\n- at hotel\\n- mall\\n- business sector\\n- yellow pages'\n",
      "' business sector. A business restaurant is likely to be'\n",
      "'\\n- business sector \\n\\nA business restaurant'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare order dependent output\n",
    "for i in range(10):\n",
    "    print(repr(prompts[i]))\n",
    "    print(repr(textOutputABCD_OD[i]))\n",
    "    print(repr(textOutputACBD_OD[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n",
      "Modify llama model to accept 2D attention mask\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES=10\n",
    "textOutputABCD=[]\n",
    "for (prefix,parallel_substrings,suffix) in prompts_parallel[:N_SAMPLES]:\n",
    "    g,t=gen_order_independent_output.genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=True, reverse_parallel_substrings_order=False, torch_device=\"cuda\")\n",
    "    textOutputABCD.append(t)\n",
    "textOutputACBD=[]\n",
    "for (prefix,parallel_substrings,suffix) in prompts_parallel[:N_SAMPLES]:\n",
    "    g,t=gen_order_independent_output.genOrderIndependentOutput(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens=10, is_order_independent=True, reverse_parallel_substrings_order=True, torch_device=\"cuda\")\n",
    "    textOutputACBD.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\\n\\n- ignore\\n- enforce\\n- authoritarian\\n- yell at\\n- avoid'\n",
      "\"\\nThe school's efforts to change were not\"\n",
      "\n",
      "'Sammy wanted to go to where the people were.  Where might he go?\\n\\n- race track\\n- populated areas\\n- the desert\\n- apartment\\n- roadblock'\n",
      "'\\nSammy wanted to go to where the people'\n",
      "\n",
      "'To locate a choker not located in a jewelry box or boutique where would you go?\\n\\n- jewelry store\\n- neck\\n- jewlery box\\n- jewelry box\\n- boutique'\n",
      "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "'Google Maps and other highway and street GPS services have replaced what?\\n\\n- united states\\n- mexico\\n- countryside\\n- atlas\\n- oceans'\n",
      "'\\n\\nThe **Google Cloud Console** is a'\n",
      "\n",
      "'The fox walked from the city into the forest, what was it looking for?\\n\\n- pretty flowers.\\n- hen house\\n- natural habitat\\n- storybook\\n- dense forest'\n",
      "'\\nThe fox walked into the forest, its'\n",
      "\n",
      "'What home entertainment equipment requires cable?\\n\\n- radio shack\\n- substation\\n- cabinet\\n- television\\n- desk'\n",
      "'\\nimport numpy as np\\nimport tensorflow as tf'\n",
      "\n",
      "'The only baggage the woman checked was a drawstring bag, where was she heading with it?\\n\\n- garbage can\\n- military\\n- jewelry store\\n- safe\\n- airport'\n",
      "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "'The forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what?\\n\\n- carpet\\n- refrigerator\\n- breadbox\\n- fridge\\n- coach'\n",
      "'\\n\\nThe 1970s were'\n",
      "\n",
      "\"What do people use to absorb extra ink from a fountain pen?\\n\\n- shirt pocket\\n- calligrapher's hand\\n- inkwell\\n- desk drawer\\n- blotter\"\n",
      "'\\n\\nThe 1960s were'\n",
      "\n",
      "'Where is a business restaurant likely to be located?\\n\\n- town\\n- at hotel\\n- mall\\n- business sector\\n- yellow pages'\n",
      "'\\nA business restaurant is likely to be located in'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare order independent output\n",
    "for i in range(N_SAMPLES):\n",
    "    print(repr(prompts[i]))\n",
    "    print(repr(textOutputABCD[i]))\n",
    "    assert(textOutputABCD[i]==textOutputACBD[i])\n",
    "    print()\n",
    "    assert(textOutputABCD[i][5:]==textOutputACBD[i][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\\n\\n- ignore\\n- enforce\\n- authoritarian\\n- yell at\\n- avoid'\n",
      "'Sammy wanted to go to where the people were.  Where might he go?\\n\\n- race track\\n- populated areas\\n- the desert\\n- apartment\\n- roadblock'\n",
      "'To locate a choker not located in a jewelry box or boutique where would you go?\\n\\n- jewelry store\\n- neck\\n- jewlery box\\n- jewelry box\\n- boutique'\n",
      "'Google Maps and other highway and street GPS services have replaced what?\\n\\n- united states\\n- mexico\\n- countryside\\n- atlas\\n- oceans'\n",
      "'The fox walked from the city into the forest, what was it looking for?\\n\\n- pretty flowers.\\n- hen house\\n- natural habitat\\n- storybook\\n- dense forest'\n",
      "'What home entertainment equipment requires cable?\\n\\n- radio shack\\n- substation\\n- cabinet\\n- television\\n- desk'\n",
      "'The only baggage the woman checked was a drawstring bag, where was she heading with it?\\n\\n- garbage can\\n- military\\n- jewelry store\\n- safe\\n- airport'\n",
      "'The forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what?\\n\\n- carpet\\n- refrigerator\\n- breadbox\\n- fridge\\n- coach'\n",
      "\"What do people use to absorb extra ink from a fountain pen?\\n\\n- shirt pocket\\n- calligrapher's hand\\n- inkwell\\n- desk drawer\\n- blotter\"\n",
      "'Where is a business restaurant likely to be located?\\n\\n- town\\n- at hotel\\n- mall\\n- business sector\\n- yellow pages'\n",
      "'Where do you put your grapes just before checking out?\\n\\n- mouth\\n- grocery cart\\n- super market\\n- fruit basket\\n- fruit market'\n",
      "'Before getting a divorce, what did the wife feel who was doing all the work?\\n\\n- harder\\n- anguish\\n- bitterness\\n- tears\\n- sadness'\n",
      "'Johnny sat on a bench and relaxed after doing a lot of work on his hobby.  Where is he?\\n\\n- state park\\n- bus depot\\n- garden\\n- gym\\n- rest area'\n",
      "\"James was cooling off two quickly.  He would die if he didn't find some way to stop what?\\n\\n- loss of heat\\n- revenge\\n- expansion\\n- relaxation\\n- calm down\"\n",
      "'Of all the rooms in a house it was his favorite, the aromas always drew him to the what?\\n\\n- yard\\n- basement\\n- kitchen\\n- living room\\n- garden'\n",
      "'Bill is stuck in marsh when a man comes up to him peaking Cajun, where is he?\\n\\n- low lands\\n- new york\\n- forest\\n- louisiana\\n- everglades'\n",
      "\"What is it called when you slowly cook using a grill?\\n\\n- backyard\\n- restaurant\\n- crockpot\\n- neighbor's house\\n- barbeque\"\n",
      "\"What type of person typically contracts illness?\\n\\n- hospital\\n- head\\n- sick person\\n- elderly person\\n- doctor's office\"\n",
      "'Where would you expect to find a pizzeria while shopping?\\n\\n- chicago\\n- street\\n- little italy\\n- food court\\n- capital cities'\n",
      "'When eating everything on the tasting menu, what does one tend to feel?\\n\\n- full stomach\\n- getting full\\n- gaining weight\\n- sick\\n- satisfaction'\n"
     ]
    }
   ],
   "source": [
    "# Print CSQA prompts\n",
    "for i in range(20):\n",
    "    print(repr(prompts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: Score llama model output for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalCSQA import eval_model_csqa_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/100=0.09 answers correct\n"
     ]
    }
   ],
   "source": [
    "eval_model_csqa_accuracy(model,tokenizer, is_order_independent=True, torch_device=\"cuda\", num_samples=100, model_name=\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/100=0.41 answers correct\n"
     ]
    }
   ],
   "source": [
    "eval_model_csqa_accuracy(model,tokenizer, is_order_independent=False, torch_device=\"cuda\", num_samples=100, model_name=\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/100=0.33 answers correct\n"
     ]
    }
   ],
   "source": [
    "eval_model_csqa_accuracy(model,tokenizer, is_order_independent=False, torch_device=\"cuda\", num_samples=100, model_name=\"llama\", reverse_parallel_substrings_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect effect of individual position id vs attention mask edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "forwardOD_1=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Override attention\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n",
      "torch.Size([7, 128]) torch.Size([7, 128])\n",
      "apply_rotary_pos_embedding: cos=torch.Size([1, 1, 7, 128]),sin=torch.Size([1, 1, 7, 128]),position_ids=tensor([[0, 1, 2, 3, 2, 3, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "forwardOI_1=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, parallel_position_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "forwardOI_2=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, reverse_parallel_substrings_order=True, parallel_position_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: for order independent output, we would expect that torch.allclse(forwardOI_1.logits[0][-1],forwardOI_2.logits[0][-1]). That is, the output logits for the last token in sequence D should be the same\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_2.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check output logits\n",
    "print(torch.allclose(forwardOI_1.logits,forwardOI_2.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2707,  0.0165,  0.2806,  ...,  1.4403,  2.0234,  0.7647],\n",
      "         [-6.0510, -1.8250,  5.9284,  ...,  0.1648, -4.5255, -0.5254],\n",
      "         [-4.7036, -9.6824, -0.7055,  ..., -3.1614, -4.1214, -4.4494],\n",
      "         ...,\n",
      "         [-5.9388, -4.7156,  2.1508,  ..., -3.5959, -6.0349, -5.1460],\n",
      "         [-7.3868, -8.1209,  3.3819,  ..., -1.0943, -6.0555, -3.4025],\n",
      "         [-8.6184, -7.9483,  1.8119,  ..., -3.5772, -6.2585, -5.3752]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 7, 32000])\n",
      "tensor([[[  0.2707,   0.0165,   0.2806,  ...,   1.4403,   2.0234,   0.7647],\n",
      "         [ -6.0510,  -1.8250,   5.9284,  ...,   0.1648,  -4.5255,  -0.5254],\n",
      "         [ -5.9388,  -4.7156,   2.1508,  ...,  -3.5959,  -6.0349,  -5.1460],\n",
      "         ...,\n",
      "         [ -4.7036,  -9.6824,  -0.7055,  ...,  -3.1614,  -4.1214,  -4.4494],\n",
      "         [ -6.4330, -10.4261,   2.0405,  ...,  -1.2784,  -4.0565,  -1.6630],\n",
      "         [ -8.6184,  -7.9483,   1.8119,  ...,  -3.5772,  -6.2585,  -5.3752]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 7, 32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Logits have rows 3/4 swapped with rows 5/6, which is reasonable given the reverse order of the parallel substrings\n",
    "print(forwardOI_1.logits,forwardOI_1.logits.shape)\n",
    "print(forwardOI_2.logits,forwardOI_2.logits.shape)\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOD_1.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_2.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 7, 128])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(forwardOI_1[\"past_key_values\"]))\n",
    "forwardOI_1[\"past_key_values\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGPT = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizerGPT = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizerGPT.pad_token_id = tokenizerGPT.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen text with attention mask with shape torch.Size([1, 1, 8, 8])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 0., 1., 1., 1., 1.]]]])\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 8, 8])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "          [1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 1.]]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m forwardOI_1_gpt\u001b[38;5;241m=\u001b[39mgenForward(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangry \u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, modelGPT, tokenizerGPT, is_order_independent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m forwardOI_2_gpt\u001b[38;5;241m=\u001b[39mgenForward(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangry \u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, modelGPT, tokenizerGPT, is_order_independent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, reverse_parallel_substrings_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwardOI_1_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforwardOI_2_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(forwardOI_1_gpt\u001b[38;5;241m.\u001b[39mpast_key_values,\u001b[38;5;28mlen\u001b[39m(forwardOI_1_gpt\u001b[38;5;241m.\u001b[39mpast_key_values))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(forwardOI_2_gpt\u001b[38;5;241m.\u001b[39mpast_key_values,\u001b[38;5;28mlen\u001b[39m(forwardOI_2_gpt\u001b[38;5;241m.\u001b[39mpast_key_values))\n",
      "\u001b[1;31mTypeError\u001b[0m: allclose(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "forwardOI_1_gpt=genForward(\" \", [\"kind \",\"angry \"], \" \", modelGPT, tokenizerGPT, is_order_independent=True, max_new_tokens=2)\n",
    "forwardOI_2_gpt=genForward(\" \", [\"kind \",\"angry \"], \" \", modelGPT, tokenizerGPT, is_order_independent=True, max_new_tokens=2, reverse_parallel_substrings_order=True)\n",
    "print(torch.allclose(forwardOI_1_gpt.past_key_values[0],forwardOI_2_gpt.past_key_values[0]))\n",
    "print(forwardOI_1_gpt.past_key_values,len(forwardOI_1_gpt.past_key_values))\n",
    "print(forwardOI_2_gpt.past_key_values,len(forwardOI_2_gpt.past_key_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True torch.Size([50257]) torch.Size([50257])\n",
      "True torch.Size([50257]) torch.Size([50257])\n",
      "True torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "False torch.Size([50257]) torch.Size([50257])\n",
      "tensor([[[-31.3032, -30.2778, -32.2991,  ..., -40.4723, -39.6234, -30.6677],\n",
      "         [-60.1542, -60.0518, -64.5230,  ..., -68.5682, -69.0504, -62.2265],\n",
      "         [-46.4999, -49.3436, -51.6780,  ..., -57.4955, -56.3297, -50.5656],\n",
      "         ...,\n",
      "         [-74.7438, -72.2002, -77.9424,  ..., -81.5106, -80.7995, -75.5365],\n",
      "         [-51.8716, -52.6931, -54.2863,  ..., -60.9329, -58.7848, -54.7333],\n",
      "         [-50.6570, -51.7063, -52.8174,  ..., -60.3168, -57.9888, -52.1964]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 8, 50257])\n",
      "tensor([[[-31.3032, -30.2778, -32.2991,  ..., -40.4723, -39.6234, -30.6677],\n",
      "         [-60.1542, -60.0518, -64.5230,  ..., -68.5682, -69.0504, -62.2265],\n",
      "         [-46.4999, -49.3436, -51.6780,  ..., -57.4955, -56.3297, -50.5656],\n",
      "         ...,\n",
      "         [-74.8620, -74.6387, -76.3108,  ..., -82.2962, -81.2576, -75.3092],\n",
      "         [-54.6795, -56.0871, -54.9103,  ..., -64.4246, -61.2044, -56.7806],\n",
      "         [-43.6068, -45.0516, -44.7986,  ..., -53.5594, -50.2085, -44.2088]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "forwardOD_1_gpt=genForward(\" \", [\"kind \",\"angry \"], \" \", modelGPT, tokenizerGPT, is_order_independent=False, max_new_tokens=2)\n",
    "print(torch.allclose(forwardOI_1_gpt.logits,forwardOD_1_gpt.logits))\n",
    "# NOTE: logits should be different for rows of tokens in sequence D\n",
    "for i in range(8):\n",
    "    print(torch.allclose(forwardOI_1_gpt.logits[0][i],forwardOD_1_gpt.logits[0][i]),forwardOI_1_gpt.logits[0][i].shape)\n",
    "print(forwardOI_1_gpt.logits,forwardOI_1_gpt.logits.shape)\n",
    "print(forwardOD_1_gpt.logits,forwardOD_1_gpt.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "False torch.Size([50257])\n",
      "True torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits vs forward(ACBD).logits for GPT - sequence D logits are the same! \n",
    "for i in range(8):\n",
    "    print(torch.allclose(forwardOI_1_gpt.logits[0][i],forwardOI_2_gpt.logits[0][i]),forwardOI_1_gpt.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits vs forward(ACBD).logits for Llama - sequence D logits are different!\n",
    "# logits are swapped for positions 2/4 and 3/5 due to reversing the order of the parallel substrings, but logits are otherwise the same. This indicates that C tokens don't attend to B tokens, as desired??\n",
    "print(torch.allclose(forwardOI_1.logits[0][2],forwardOI_2.logits[0][4]))\n",
    "print(torch.allclose(forwardOI_1.logits[0][3],forwardOI_2.logits[0][5])) \n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_2.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits order independent vs forward(ABCD).logits order dependent for Llama - sequence D logits are different!\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOD_1.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Gen text with attention mask with shape torch.Size([1, 1, 7, 7]), position_ids tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "Position IDs=tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits parallel attention mask but default position ids vs forward(ABCD).logits parallel attention mask and parallel position ids for Llama\n",
    "forwardOI_1_default_position=genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=True, max_new_tokens=2, reverse_parallel_substrings_order=False,parallel_position_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modify llama model to accept 2D attention mask\n",
      "Position IDs=tensor([[0, 1, 2, 3, 2, 3, 4]])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1]]]], dtype=torch.int32)\n",
      "4D attention mask torch.Size([1, 1, 7, 7])\n",
      "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           -3.4028e+38, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00, -3.4028e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Basically, does inputting custom position_ids change the output logits, while keeping default attention mask constant?\n",
    "# Only C,D logits are different, which is expected because the position_ids are different\n",
    "forwardOD_1_custom_position = genForward(\" \", [\"kind \",\"angry \"], \" \", modelLlama, tokenizerLlama, is_order_independent=False, max_new_tokens=2, reverse_parallel_substrings_order=False,parallel_position_ids=True)\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOD_1.logits[0][i],forwardOD_1_custom_position.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# forward(ABCD).logits order independent vs forward(ABCD).logits order dependent for Llama - sequence D logits are ???\n",
    "# Basically, does inputting custom position ids change the output logits? It should, for C/D tokens only, right?\n",
    "# We'd expect that inputting custom position ids tensor([[0, 1, 2, 3, 2, 3, 4]] instead of default tensor([[0, 1, 2, 3, 4, 5, 6]] would change output logits for C and D\n",
    "# Instead, inputting custom position ids changes the output logits for B,C, and D tokens. Why do custom position ids affect the output logits for B tokens?\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOI_1.logits[0][i],forwardOI_1_default_position.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Does inputting a custom attention mask change the output logits for C/D tokens only? Default position ids for both\n",
    "# Changing the attention mask changes the output logits for tokens C,D only as expected\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOD_1.logits[0][i],forwardOI_1_default_position.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "True torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n",
      "False torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "# Basically, intervening on both attention mask and position ids affects only the C,D output logits\n",
    "for i in range(7):\n",
    "    print(torch.allclose(forwardOD_1.logits[0][i],forwardOI_1.logits[0][i]),forwardOI_1.logits[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n",
      "torch.Size([1, 12, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "print(len(forwardOI_1_gpt.past_key_values))\n",
    "print(len(forwardOI_1_gpt.past_key_values[0]))\n",
    "print(forwardOI_1_gpt.past_key_values[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(forwardOI_1_gpt.past_key_values)):\n",
    "    for j in range(len(forwardOI_1_gpt.past_key_values[i])):\n",
    "        print(torch.allclose(forwardOI_1_gpt.past_key_values[i][j],forwardOI_2_gpt.past_key_values[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
